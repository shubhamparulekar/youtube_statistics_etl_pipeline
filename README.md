## YouTube Data Engineering Pipeline 

A complete end-to-end data engineering project that extracts YouTube data, processes it through an ETL pipeline, and orchestrates it with Apache Airflow - all running locally and completely free. 

### ğŸ“‹ Project Overview 
* Extracts data from YouTube Data API v3
* Transforms and cleans the data
* Loads processed data to local storage
* Orchestrates the entire process with Apache Airflow
* Provides analysis and insights
     

### ğŸ—ï¸ Project Architecture 
```
YouTube API â†’ Extract â†’ Transform â†’ Load â†’ Airflow Orchestration â†’ Analysis
```

### Data Flow: 
* Extract: Pull video data from YouTube API based on search queries
* Transform: Clean data, convert data types, create derived columns
* Load: Save processed data to CSV files
* Orchestrate: Apache Airflow manages the pipeline execution
* Analyze: Jupyter notebooks provide data insights

### ğŸ“ Project Structure
```
youtube-data-pipeline/
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py              # Environment and path configuration
â”œâ”€â”€ scripts/                    # ETL scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract_youtube_data.py # Extract data from YouTube API
â”‚   â”œâ”€â”€ transform_data.py       # Clean and transform data
â”‚   â””â”€â”€ load_data.py           # Save final processed data
â”œâ”€â”€ dags/                       # Airflow DAG definitions
â”‚   â””â”€â”€ youtube_etl_dag.py     # Main ETL pipeline DAG
â”œâ”€â”€ research/                   # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ api_exploration.ipynb  # API testing and exploration
â”‚   â””â”€â”€ data_analysis.ipynb    # Data analysis and visualization
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                   # Raw JSON data from API
â”‚   â”œâ”€â”€ processed/             # Cleaned CSV data
â”‚   â””â”€â”€ youtube_data_final.csv # Final consolidated data
â”œâ”€â”€ tests/                      # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_extract.py        # Tests for extract functionality
â”‚   â””â”€â”€ test_transform.py      # Tests for transform functionality
â”œâ”€â”€ airflow_home/              # Airflow installation directory
â”‚   â”œâ”€â”€ dags/                  # Airflow DAGs (symlink or copy)
â”‚   â”œâ”€â”€ logs/                  # Airflow logs
â”‚   â””â”€â”€ plugins/               # Airflow plugins
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ main.py                   # Main pipeline runner
â”œâ”€â”€ .gitignore                # Git ignore file
â””â”€â”€ README.md                 # This file
```

### ğŸš€ Getting Started 
**Prerequisites** 
* Python 3.8+
* Virtual environment (recommended)
* Google Cloud account (free tier)
* YouTube Data API v3 enabled
     

**Installation** 
* Clone the repository:
     
```
git clone <your-repo-url>
cd youtube-data-pipeline
```
 
* Create virtual environment:

```
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
 
* Install dependencies:
```
pip install -r requirements.txt
```
 
** Set up YouTube API: 
** Go to Google Cloud Console 
** Create a new project or select existing one
** Enable YouTube Data API v3
** Create API credentials (API Key)
** Copy your API key
         
* Configure environment variables:
  Create a .env file in the project root:

  ```
     YOUTUBE_API_KEY=your_actual_api_key_here
     DEFAULT_QUERY=data science
     DEFAULT_MAX_RESULTS=10
     AIRFLOW_DAG_SCHEDULE=@daily
  ```
     

